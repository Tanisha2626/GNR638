{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled8.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOeGfylMIeu2m4BfMRkxbE1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tanisha2626/GNR638/blob/main/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlelqJNS8gWZ"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as Fn\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from torchvision.utils import make_grid\n",
        "from lib.Network_Res2Net_GRA_NCD import Network\n",
        "from utils.data_val import get_loader, test_dataset\n",
        "from utils.utils import clip_gradient, adjust_lr\n",
        "from tensorboardX import SummaryWriter\n",
        "import logging\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "\"\"\"\n",
        "SINET-v2 presented the first systematic study on Concealed Object Detection (COD), which aims to identify objects that are visually embedded in their background.\n",
        "This paper detects camouflaged objects in an image which are often missed by the standard networks for object detection, image segmentation like YoLO & RCNN family\n",
        "\"\"\"\n",
        "\n",
        "def structure_loss(pred, mask):\n",
        "    \"\"\"\n",
        "    The loss consists of 2 componenets i.e. wbce & wiou \n",
        "    wbce captures the objectness score of the detected concealed object\n",
        "    wiou captures the intersection over union of the camoflaged object with the mask\n",
        "    \"\"\"\n",
        "    weit = 1 + 5 * torch.abs(Fn.avg_pool2d(mask, kernel_size=31, stride=1, padding=15) - mask)\n",
        "    wbce = Fn.binary_cross_entropy_with_logits(pred, mask, reduce='none')\n",
        "    wbce = (weit * wbce).sum(dim=(2, 3)) / weit.sum(dim=(2, 3))\n",
        "\n",
        "    pred = torch.sigmoid(pred)\n",
        "    inter = ((pred * mask) * weit).sum(dim=(2, 3))\n",
        "    union = ((pred + mask) * weit).sum(dim=(2, 3))\n",
        "    wiou = 1 - (inter + 1) / (union - inter + 1)\n",
        "    return (wbce + wiou).mean()\n",
        "\n",
        "\n",
        "def train(train_loader, model, optimizer, epoch, save_path, writer):\n",
        "    \"\"\"\n",
        "    Model training pipeline :\n",
        "    The network architecture comprises of 4 modules :\n",
        "    1) Feature Extractor\n",
        "    2) Texture Enhancement Module: to capture fine-grained textures with the enlarged context cues\n",
        "    4) Neighbouring Connection Decoder\n",
        "    5) Group Reversal Attention:\n",
        "    \"\"\"\n",
        "    global step\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    epoch_step = 0\n",
        "    try:\n",
        "        for i, (images, gts) in enumerate(train_loader, start=1):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            images = images.cuda()\n",
        "            gts = gts.cuda()\n",
        "\n",
        "            preds = model(images)\n",
        "            initial_loss = structure_loss(preds[0], gts) + structure_loss(preds[1], gts) + structure_loss(preds[2], gts)\n",
        "            final_loss = structure_loss(preds[3], gts)\n",
        "\n",
        "            loss = initial_loss + final_loss\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            clip_gradient(optimizer, opt.clip)\n",
        "            optimizer.step()\n",
        "\n",
        "            step += 1\n",
        "            epoch_step += 1\n",
        "            total_loss += loss.data\n",
        "\n",
        "            if i % 20 == 0 or i == total_step or i == 1:\n",
        "                print('{} Epoch [{:03d}/{:03d}], Step [{:04d}/{:04d}], Total_loss: {:.4f} Loss1: {:.4f} Loss2: {:0.4f}'.\n",
        "                      format(datetime.now(), epoch, opt.epoch, i, total_step, loss.data, initial_loss.data, final_loss.data))\n",
        "                logging.info(\n",
        "                    '[Train Info]:Epoch [{:03d}/{:03d}], Step [{:04d}/{:04d}], Total_loss: {:.4f} Loss1: {:.4f} '\n",
        "                    'Loss2: {:0.4f}'.\n",
        "                    format(epoch, opt.epoch, i, total_step, loss.data, initial_loss.data, final_loss.data))\n",
        "                # TensorboardX-Loss\n",
        "                writer.add_scalars('Loss_Statistics',\n",
        "                                   {'Loss_init': initial_loss.data, 'Loss_final': final_loss.data,\n",
        "                                    'Loss_total': loss.data},\n",
        "                                   global_step=step)\n",
        "                # TensorboardX-Training Data\n",
        "                grid_image = make_grid(images[0].clone().cpu().data, 1, normalize=True)\n",
        "                writer.add_image('RGB', grid_image, step)\n",
        "                grid_image = make_grid(gts[0].clone().cpu().data, 1, normalize=True)\n",
        "                writer.add_image('GT', grid_image, step)\n",
        "\n",
        "                # TensorboardX-Outputs\n",
        "                res = preds[0][0].clone()\n",
        "                res = res.sigmoid().data.cpu().numpy().squeeze()\n",
        "                res = (res - res.min()) / (res.max() - res.min() + 1e-8)\n",
        "                writer.add_image('Pred_init', torch.tensor(res), step, dataformats='HW')\n",
        "                res = preds[3][0].clone()\n",
        "                res = res.sigmoid().data.cpu().numpy().squeeze()\n",
        "                res = (res - res.min()) / (res.max() - res.min() + 1e-8)\n",
        "                writer.add_image('Pred_final', torch.tensor(res), step, dataformats='HW')\n",
        "\n",
        "        total_loss /= epoch_step\n",
        "        logging.info('[Train Info]: Epoch [{:03d}/{:03d}], Loss_AVG: {:.4f}'.format(epoch, opt.epoch, total_loss))\n",
        "        writer.add_scalar('Loss-epoch', total_loss, global_step=epoch)\n",
        "        if epoch % 50 == 0:\n",
        "            torch.save(model.state_dict(), save_path + 'Net_epoch_{}.pth'.format(epoch))\n",
        "    except KeyboardInterrupt:\n",
        "        print('Keyboard Interrupt: save model and exit.')\n",
        "        if not os.path.exists(save_path):\n",
        "            os.makedirs(save_path)\n",
        "        torch.save(model.state_dict(), save_path + 'Net_epoch_{}.pth'.format(epoch + 1))\n",
        "        print('Save checkpoints successfully!')\n",
        "        raise\n",
        "\n",
        "\n",
        "def val(test_loader, model, epoch, save_path, writer):\n",
        "    \"\"\"\n",
        "    Model validation pipeline :\n",
        "    Output the MAE error in the co-ordinates of true & detected object over the validation set\n",
        "    \"\"\"\n",
        "    global best_mae, best_epoch\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        mae_sum = 0\n",
        "        for i in range(test_loader.size):\n",
        "            image, gt, name, img_for_post = test_loader.load_data()\n",
        "            gt = np.asarray(gt, np.float32)\n",
        "            gt /= (gt.max() + 1e-8)\n",
        "            image = image.cuda()\n",
        "\n",
        "            res = model(image)\n",
        "\n",
        "            res = Fn.upsample(res[3], size=gt.shape, mode='bilinear', align_corners=False)\n",
        "            res = res.sigmoid().data.cpu().numpy().squeeze()\n",
        "            res = (res - res.min()) / (res.max() - res.min() + 1e-8)\n",
        "            mae_sum += np.sum(np.abs(res - gt)) * 1.0 / (gt.shape[0] * gt.shape[1])\n",
        "        mae = mae_sum / test_loader.size\n",
        "        writer.add_scalar('MAE', torch.tensor(mae), global_step=epoch)\n",
        "        print('Epoch: {}, MAE: {}, bestMAE: {}, bestEpoch: {}.'.format(epoch, mae, best_mae, best_epoch))\n",
        "        if epoch == 1:\n",
        "            best_mae = mae\n",
        "        else:\n",
        "            if mae < best_mae:\n",
        "                best_mae = mae\n",
        "                best_epoch = epoch\n",
        "                torch.save(model.state_dict(), save_path + 'Net_epoch_best.pth')\n",
        "                print('Save state_dict successfully! Best epoch:{}.'.format(epoch))\n",
        "        logging.info(\n",
        "            '[Val Info]:Epoch:{} MAE:{} bestEpoch:{} bestMAE:{}'.format(epoch, mae, best_epoch, best_mae))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--epoch', type=int, default=100, help='epoch number')\n",
        "    parser.add_argument('--lr', type=float, default=1e-4, help='learning rate')\n",
        "    parser.add_argument('--batchsize', type=int, default=36, help='training batch size')\n",
        "    parser.add_argument('--trainsize', type=int, default=352, help='training dataset size')\n",
        "    parser.add_argument('--clip', type=float, default=0.5, help='gradient clipping margin')\n",
        "    parser.add_argument('--decay_rate', type=float, default=0.1, help='decay rate of learning rate')\n",
        "    parser.add_argument('--decay_epoch', type=int, default=50, help='every n epochs decay learning rate')\n",
        "    parser.add_argument('--load', type=str, default=None, help='train from checkpoints')\n",
        "    parser.add_argument('--gpu_id', type=str, default='0', help='train use gpu')\n",
        "    parser.add_argument('--train_root', type=str, default='./Dataset/TrainValDataset/',\n",
        "                        help='the training rgb images root')\n",
        "    parser.add_argument('--val_root', type=str, default='./Dataset/TestDataset/CAMO/',\n",
        "                        help='the test rgb images root')\n",
        "    parser.add_argument('--save_path', type=str,\n",
        "                        default='./snapshot/SINet_V2/',\n",
        "                        help='the path to save model and log')\n",
        "    opt = parser.parse_args()\n",
        "\n",
        "    # set the device for training\n",
        "    if opt.gpu_id == '0':\n",
        "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "        print('USE GPU 0')\n",
        "    elif opt.gpu_id == '1':\n",
        "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
        "        print('USE GPU 1')\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "    # build the model\n",
        "    model = Network(channel=32).cuda()\n",
        "\n",
        "    if opt.load is not None:\n",
        "        model.load_state_dict(torch.load(opt.load))\n",
        "        print('load model from ', opt.load)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), opt.lr)\n",
        "\n",
        "    save_path = opt.save_path\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "\n",
        "    # load data\n",
        "    print('load data...')\n",
        "    train_loader = get_loader(image_root=opt.train_root + 'Imgs/',\n",
        "                              gt_root=opt.train_root + 'GT/',\n",
        "                              batchsize=opt.batchsize,\n",
        "                              trainsize=opt.trainsize,\n",
        "                              num_workers=8)\n",
        "    val_loader = test_dataset(image_root=opt.val_root + 'Imgs/',\n",
        "                              gt_root=opt.val_root + 'GT/',\n",
        "                              testsize=opt.trainsize)\n",
        "    total_step = len(train_loader)\n",
        "\n",
        "    # logging\n",
        "    logging.basicConfig(filename=save_path + 'log.log',\n",
        "                        format='[%(asctime)s-%(filename)s-%(levelname)s:%(message)s]',\n",
        "                        level=logging.INFO, filemode='a', datefmt='%Y-%m-%d %I:%M:%S %p')\n",
        "    logging.info(\"Network-Train\")\n",
        "    logging.info('Config: epoch: {}; lr: {}; batchsize: {}; trainsize: {}; clip: {}; decay_rate: {}; load: {}; '\n",
        "                 'save_path: {}; decay_epoch: {}'.format(opt.epoch, opt.lr, opt.batchsize, opt.trainsize, opt.clip,\n",
        "                                                         opt.decay_rate, opt.load, save_path, opt.decay_epoch))\n",
        "\n",
        "    step = 0\n",
        "    writer = SummaryWriter(save_path + 'summary')\n",
        "    best_mae = 1\n",
        "    best_epoch = 0\n",
        "\n",
        "    print(\"Training loop begins...\")\n",
        "    for epoch in range(1, opt.epoch):\n",
        "        cur_lr = adjust_lr(optimizer, opt.lr, epoch, opt.decay_rate, opt.decay_epoch)\n",
        "        writer.add_scalar('learning_rate', cur_lr, global_step=epoch)\n",
        "        train(train_loader, model, optimizer, epoch, save_path, writer)\n",
        "        val(val_loader, model, epoch, save_path, writer)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}